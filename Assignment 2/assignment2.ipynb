{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this question we are asked to sort the emergency calls based on their probability of being an actual emergency. To do that, we have a slick rule called Bayes' Theorem, something derived by our friendly neighborhood statistician, Thomas Bayes. Which states:\n",
    "\n",
    "$$\n",
    "P(A|B)\n",
    "=\n",
    "\\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "Where $A$ and $B$ are events. Explaining this theorem in detail:\n",
    "- $P(A|B)$ is the conditional probability of $A$ given $B$.\n",
    "- $P(B|A)$ is interpreted in a similar fashion with the former one.\n",
    "- $P(A) \\text{ and } P(B)$ are probabilities of $A \\text{ and } B$ without any given conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or if we were to make a more general definition:\n",
    "\n",
    "$\\text{Let } A_1, A_2,...,A_k$ be a collection of $k$ mutually exclusive and exhaustive events with $prior$ probabilities $P(A_i) \\text{ }(i = 1,...,k)$. Then for any other event $B$ for which $P(B) > 0$, the $posterior$ probability of $A_j$ given that $B$ has occured is:\n",
    "\n",
    "\n",
    "$$\n",
    "P(A_j|B)\n",
    "=\n",
    "\\frac{P(B|A_j)\\cdot P(A_j)}{\\sum_{i=1}^{k} P(B|A_i)\\cdot P(A_i)}\n",
    "\\qquad\n",
    "j = 1, ... , k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine all cases one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have these values:\n",
    "- $P(D) = 0.01 \\qquad$ Probability of dangerous fire\n",
    "- $P(\\text{Barbecue Smoke}) = 0.2$ \n",
    "- $P(S|D) = 0.8\\qquad$ Probability of smoke given that there is a dangerous fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Bayes' Theorem, we need to solve this:\n",
    "\n",
    "$$\n",
    "P(D|S)\n",
    "=\n",
    "\\frac{P(S|D) \\cdot P(D)}{P(S)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we don't know the value of $P(S)$. But we can calculate it with the usage of law of total probability, assuming smoke is caused by only barbecue or a dangerous fire:\n",
    "\n",
    "$$\n",
    "P(S) = P(\\text{Barbecue Smoke}) + P(S|D) \\cdot P(D)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.2 + (0.8 \\times 0.01)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.208\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now substituting other values:\n",
    "\n",
    "$$\n",
    "P(D|S)\n",
    "=\n",
    "\\frac{0.8 \\times 0.01}{0.208}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx  0.0385\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say for this case, the probability of a dangerous fire is approximately 3.85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have these values:\n",
    "- $P(D) = 0.35 \\qquad$ Probability of dangerous fire\n",
    "- $P(\\text{Factory Smoke}) = 0.1$ \n",
    "- $P(S|D) = 0.01\\qquad$ Probability of smoke given that there is a dangerous fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $P(S)$: (Main Bayes' Theorem is the same, so it won't take place here explicitly.)\n",
    "\n",
    "$$\n",
    "P(S) = P(\\text{Factory Smoke}) + P(S|D) \\cdot P(D)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.1 + (0.35 \\times 0.01)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.1035\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now substituting other values:\n",
    "\n",
    "$$\n",
    "P(D|S)\n",
    "=\n",
    "\\frac{0.35 \\times 0.01}{0.1035}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx  0.0338\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd Neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have these values:\n",
    "- $P(D) = 0.1 \\qquad$ Probability of dangerous fire\n",
    "- $P(\\text{Coal Smoke}) = 0.8$ \n",
    "- $P(S|D) = 0.3\\qquad$ Probability of smoke given that there is a dangerous fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating $P(S)$:\n",
    "\n",
    "$$\n",
    "P(S) = P(\\text{Coal Smoke}) + P(S|D) \\cdot P(D)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.8 + (0.3 \\times 0.1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.83\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now substituting other values:\n",
    "\n",
    "$$\n",
    "P(D|S)\n",
    "=\n",
    "\\frac{0.3 \\times 0.1}{0.83}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\approx  0.0361\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing probabilities for all three cases $(P_1, P_2, P_3)$, we can sort them in this way:\n",
    "\n",
    "$$\n",
    "P_1 > P_3 > P_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the boxes first:\n",
    "\n",
    "| Box Name | Red Count | Blue Count | Pr. of Being Selected |\n",
    "|----------|-----------|------------|-----------------------|\n",
    "|   Box 1  |     5     |      3     |          0.4          |\n",
    "|   Box 2  |     7     |      4     |          0.6          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of a blue ball drawn is the sum of separate probabilities of a blue ball drawn from each box. For the first box:\n",
    "\n",
    "$$\n",
    "0.4 \\times \\frac{3}{8} = \\frac{1.2}{8}\n",
    "$$\n",
    "\n",
    "For the second one:\n",
    "\n",
    "$$\n",
    "0.6 \\times \\frac{4}{11} = \\frac{2.4}{11}\n",
    "$$\n",
    "\n",
    "Which makes a total of:\n",
    "\n",
    "$$\n",
    "\\frac{1.2}{8} + \\frac{2.4}{11} = \\frac{32.4}{88}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of second box being selected ($P(S)$), if it is known that a blue ball is drawn ($P(B)$) can be calculated using Bayes' Theorem again:\n",
    "\n",
    "$$\n",
    "P(S|B) = \\frac{P(B|S) \\cdot P(S)}{P(B)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$P(B)$ was calculated as $\\frac{32.4}{88}$ in the previous case. We already know other values too. Substituting:\n",
    "\n",
    "$$\n",
    "P(S|B) = \\frac{\\frac{4}{11} \\cdot 0.6}{\\frac{32.4}{88}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{19.2}{32.4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text classification is the primary application for Naïve Bayes classifier methods. (<span style=\"color: red;\">T</span>) <span style=\"color: red;\">Yes, it is mostly used for spam detection, sentiment analysis etc.</span>\n",
    "- When an attribute value in the testing record has no example in the training set, the total posterior probability in a Naïve Bayes algorithm will be zero. (<span style=\"color: red;\">T</span>) <span style=\"color: red;\">True, and it's called the Zero Frequency Problem.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II: Movie Review Classification System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing and Visualizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'aclImdb/train/'\n",
    "TEST_PATH = 'aclImdb/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we should acquire the data. So that we can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Function to load the data from the aclImdb dataset.\n",
    "\n",
    "    :param path: Path to the train or test folder\n",
    "    :return: A pandas DataFrame with the reviews and the sentiment\n",
    "    \"\"\"\n",
    "\n",
    "    data = {'review': [], 'sentiment': []}\n",
    "    \n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        sentiment_path = os.path.join(path, sentiment)\n",
    "        for filename in os.listdir(sentiment_path):\n",
    "            if filename.endswith('.txt'):\n",
    "                with open(os.path.join(sentiment_path, filename), 'r', encoding='utf-8') as file:\n",
    "                    data['review'].append(file.read())\n",
    "                    data['sentiment'].append(sentiment)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(TRAIN_PATH)\n",
    "test_data = load_data(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  Bromwell High is a cartoon comedy. It ran at t...       pos\n",
      "1  I liked the film. Some of the action scenes we...       pos\n",
      "2  Somewhat funny and well-paced action thriller ...       pos\n",
      "3  Just two comments....SEVEN years apart? Hardly...       pos\n",
      "4  Another Aussie masterpiece, this delves into t...       pos\n",
      "\n",
      "                                              review sentiment\n",
      "0  I went and saw this movie last night after bei...       pos\n",
      "1  Actor turned director Bill Paxton follows up h...       pos\n",
      "2  As a recreational golfer with some knowledge o...       pos\n",
      "3  I saw this film in a sneak preview, and it is ...       pos\n",
      "4  Bill Paxton has taken the true story of the 19...       pos\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print()\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I LOVE Jack's jokes like 'The cliché is...' or \"Over the top cliché guy, black, oily skin, kinda spooky...\". He is just hilarious! Daniel's starting to catch up on him to! Good thing Jack's not on the team anymore (in a way) or else it would have been sarcasm mania!!!!I just love all the plots (season 8, a little less, I have to admit), the characters are great, the actors are great, I'm starting to pick up facial expressions (and more) from Jack, Daniel and Teal'c...It just all theoretically possible and exciting...oops! Their I go again!!! Sorry, I'm also starting to pick up traits from Carter, and all of this is driving my parents NUTZ!!!!!!! Well, to conclude, I think it's good for another three seasons or so, especially if they keep on packing the episodes with all this humor, drama, action and so forth!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "# an example review\n",
    "print(train_data.iloc[13]['review'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of the data to better understand it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "colors = ['#024CAA', '#FF7F0E']\n",
    "\n",
    "sns.countplot(x='sentiment', data=train_data, ax=axes[0], palette=colors, hue='sentiment', legend=False, edgecolor='black', linewidth=2) \n",
    "sns.countplot(x='sentiment', data=test_data, ax=axes[1], palette=colors, hue='sentiment', legend=False, edgecolor='black', linewidth=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: BoW, Model and All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining stop words\n",
    "stop_words = set([\n",
    "    'the', 'is', 'and', 'in', 'to', 'of', 'a', 'i', 'it', 'that', 'on', 'for', 'with', 'as', \n",
    "    'this', 'was', 'but', 'be', 'at', 'by', 'an', 'he', 'she', 'they', 'them', 'we', 'you',\n",
    "    'are', 'from', 'or', 'not', 'if', 'have', 'has', 'had', 'my', 'your', 'his', 'her', 'its',\n",
    "    'which', 'so', 'there', 'will', 'would', 'can', 'could', 'our', 'their', 'what', 'when', \n",
    "    'where', 'who', 'how', 'just', 'about', 'more', 'up', 'out', 'into', 'no', 'than', 'some', \n",
    "    'other', 'only', 'such', 'do', 'over', 'did'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My simple tokenizer which will pass non-alphabetic characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    A simple function to tokenize a text.\n",
    "\n",
    "    :param text: A text\n",
    "    :return: A list of words\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # ignoring non-alphabetic characters\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My logic of BoW. I had so much confusion with it at first, especially after examining the LIBSVM format but I decided on this simple format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(data: pd.DataFrame, ngram: int=1) -> tuple[dict[str, int], dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Function to create a bag of words from a list of reviews.\n",
    "\n",
    "    :param data: A pandas DataFrame with the reviews and the sentiment\n",
    "    :param ngram: The n-gram order (default=1)\n",
    "    :return: A tuple with the positive and negative dictionaries\n",
    "    \"\"\"\n",
    "    pos_dict = defaultdict(int)\n",
    "    neg_dict = defaultdict(int)\n",
    "    \n",
    "    # over reviews and sentiments\n",
    "    for _, row in data.iterrows():\n",
    "        tokens = tokenize(row['review'])\n",
    "        sentiment = row['sentiment']\n",
    "\n",
    "        # ngrams = []\n",
    "        # for i in range(len(words) - ngram + 1):\n",
    "        #     ngrams.append(tuple(words[i:i + ngram]))\n",
    "        \n",
    "        # kinda syntactic sugar to create n-grams\n",
    "        ngrams = zip(*[tokens[i:] for i in range(ngram)])  \n",
    "\n",
    "        for ngram_tuple in ngrams:\n",
    "            ngram_str = ' '.join(ngram_tuple) # join tuple to form the n-gram string\n",
    "            if sentiment == 'pos':\n",
    "                pos_dict[ngram_str] += 1\n",
    "            else:\n",
    "                neg_dict[ngram_str] += 1\n",
    "\n",
    "    return dict(pos_dict), dict(neg_dict)\n",
    "\n",
    "unigram_train_pos, unigram_train_neg = bow(train_data, ngram=1)\n",
    "bigram_train_pos, bigram_train_neg = bow(train_data, ngram=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bromwell': 8,\n",
       " 'high': 725,\n",
       " 'cartoon': 173,\n",
       " 'comedy': 1145,\n",
       " 'ran': 82,\n",
       " 'same': 1466,\n",
       " 'time': 4445,\n",
       " 'programs': 30,\n",
       " 'school': 526,\n",
       " 'life': 2851,\n",
       " 'teachers': 22,\n",
       " 'years': 2019,\n",
       " 'teaching': 29,\n",
       " 'profession': 28,\n",
       " 'lead': 447,\n",
       " 'me': 3707,\n",
       " 'believe': 769,\n",
       " 'highs': 9,\n",
       " 'satire': 103,\n",
       " 'much': 3404,\n",
       " 'closer': 96,\n",
       " 'reality': 426,\n",
       " 'scramble': 2,\n",
       " 'survive': 86,\n",
       " 'financially': 12,\n",
       " 'insightful': 32,\n",
       " 'students': 125,\n",
       " 'see': 4303,\n",
       " 'right': 1196,\n",
       " 'through': 1806,\n",
       " 'pathetic': 45,\n",
       " 'pomp': 4,\n",
       " 'pettiness': 2,\n",
       " 'whole': 970,\n",
       " 'situation': 256,\n",
       " 'all': 8553,\n",
       " 'remind': 62,\n",
       " 'schools': 29,\n",
       " 'knew': 345,\n",
       " 'saw': 1233,\n",
       " 'episode': 740,\n",
       " 'student': 113,\n",
       " 'repeatedly': 34,\n",
       " 'tried': 192,\n",
       " 'burn': 24,\n",
       " 'down': 1133,\n",
       " 'immediately': 174,\n",
       " 'recalled': 6,\n",
       " 'classic': 866,\n",
       " 'line': 582,\n",
       " 'inspector': 64,\n",
       " 'im': 1380,\n",
       " 'here': 1922,\n",
       " 'sack': 7,\n",
       " 'one': 9615,\n",
       " 'welcome': 94,\n",
       " 'expect': 427,\n",
       " 'many': 2793,\n",
       " 'adults': 151,\n",
       " 'age': 493,\n",
       " 'think': 2662,\n",
       " 'far': 967,\n",
       " 'fetched': 14,\n",
       " 'pity': 78,\n",
       " 'isnt': 959,\n",
       " 'liked': 656,\n",
       " 'film': 14459,\n",
       " 'action': 1206,\n",
       " 'scenes': 1739,\n",
       " 'were': 3671,\n",
       " 'very': 6056,\n",
       " 'interesting': 1063,\n",
       " 'tense': 77,\n",
       " 'well': 4094,\n",
       " 'done': 1099,\n",
       " 'especially': 1114,\n",
       " 'opening': 328,\n",
       " 'scene': 1790,\n",
       " 'semi': 4,\n",
       " 'truck': 32,\n",
       " 'seemed': 360,\n",
       " 'donebr': 34,\n",
       " 'br': 20461,\n",
       " 'transitional': 6,\n",
       " 'filmed': 280,\n",
       " 'ways': 355,\n",
       " 'lapse': 4,\n",
       " 'photography': 175,\n",
       " 'unusual': 157,\n",
       " 'colors': 91,\n",
       " 'angles': 78,\n",
       " 'also': 4065,\n",
       " 'funny': 1422,\n",
       " 'several': 539,\n",
       " 'parts': 462,\n",
       " 'evil': 472,\n",
       " 'guy': 704,\n",
       " 'portrayed': 236,\n",
       " 'too': 2561,\n",
       " 'id': 451,\n",
       " 'give': 1108,\n",
       " 'somewhat': 395,\n",
       " 'wellpaced': 8,\n",
       " 'thriller': 291,\n",
       " 'jamie': 59,\n",
       " 'foxx': 23,\n",
       " 'hapless': 23,\n",
       " 'fasttalking': 6,\n",
       " 'hoodlum': 5,\n",
       " 'chosen': 87,\n",
       " 'overly': 77,\n",
       " 'demanding': 26,\n",
       " 'us': 1730,\n",
       " 'treasury': 11,\n",
       " 'agent': 149,\n",
       " 'david': 475,\n",
       " 'morse': 27,\n",
       " 'released': 403,\n",
       " 'streets': 122,\n",
       " 'new': 1815,\n",
       " 'york': 353,\n",
       " 'find': 1614,\n",
       " 'picky': 10,\n",
       " 'computer': 124,\n",
       " 'thiefhacker': 1,\n",
       " 'doug': 15,\n",
       " 'hutchinson': 6,\n",
       " 'stole': 42,\n",
       " 'fortytwo': 1,\n",
       " 'million': 117,\n",
       " 'dollars': 49,\n",
       " 'left': 703,\n",
       " 'two': 2670,\n",
       " 'guards': 11,\n",
       " 'shot': 638,\n",
       " 'deadbr': 12,\n",
       " 'bait': 12,\n",
       " 'marks': 55,\n",
       " 'sophomore': 7,\n",
       " 'feature': 310,\n",
       " 'antoine': 6,\n",
       " 'fuqua': 2,\n",
       " 'replacement': 20,\n",
       " 'killers': 103,\n",
       " 'handles': 31,\n",
       " 'task': 70,\n",
       " 'fairly': 214,\n",
       " 'even': 3600,\n",
       " 'though': 1761,\n",
       " 'doesnt': 1432,\n",
       " 'top': 629,\n",
       " 'first': 3465,\n",
       " 'movie': 13256,\n",
       " 'films': 3130,\n",
       " 'common': 186,\n",
       " 'sequences': 256,\n",
       " 'flatout': 5,\n",
       " 'excellentbr': 22,\n",
       " 'pretty': 1083,\n",
       " 'good': 5481,\n",
       " 'although': 1082,\n",
       " 'character': 2391,\n",
       " 'annoying': 143,\n",
       " 'beginning': 548,\n",
       " 'throughout': 555,\n",
       " 'began': 133,\n",
       " 'catch': 188,\n",
       " 'marvelous': 105,\n",
       " 'mastermind': 16,\n",
       " 'ruthless': 39,\n",
       " 'john': 982,\n",
       " 'malkovich': 4,\n",
       " 'patient': 63,\n",
       " 'late': 508,\n",
       " 'laurence': 47,\n",
       " 'olivier': 52,\n",
       " 'marathon': 17,\n",
       " 'man': 2217,\n",
       " 'okay': 155,\n",
       " 'comes': 932,\n",
       " 'ingenious': 41,\n",
       " 'plan': 121,\n",
       " 'get': 3123,\n",
       " 'whoever': 42,\n",
       " 'cost': 72,\n",
       " 'commentsseven': 1,\n",
       " 'apart': 201,\n",
       " 'hardly': 174,\n",
       " 'evidence': 68,\n",
       " 'relentless': 23,\n",
       " 'pullingpower': 1,\n",
       " 'been': 3053,\n",
       " 'mentioned': 192,\n",
       " 'lowbudget': 61,\n",
       " 'telemovie': 2,\n",
       " 'status': 71,\n",
       " 'gantry': 8,\n",
       " 'row': 60,\n",
       " 'mitigating': 1,\n",
       " 'factor': 69,\n",
       " 'limited': 125,\n",
       " 'appeal': 139,\n",
       " 'having': 930,\n",
       " 'said': 711,\n",
       " 'however': 1301,\n",
       " 'thing': 1176,\n",
       " 'without': 1240,\n",
       " 'merit': 23,\n",
       " 'either': 481,\n",
       " 'entertainment': 339,\n",
       " 'fright': 20,\n",
       " 'outing': 31,\n",
       " 'per': 47,\n",
       " 'sebr': 1,\n",
       " 'true': 1024,\n",
       " 'plot': 1686,\n",
       " 'most': 3542,\n",
       " 'basic': 189,\n",
       " 'reworking': 11,\n",
       " 'amityville': 4,\n",
       " 'horror': 851,\n",
       " 'case': 538,\n",
       " 'intrigue': 46,\n",
       " 'gibney': 3,\n",
       " 'might': 922,\n",
       " 'made': 2698,\n",
       " 'worthwhile': 68,\n",
       " 'impression': 141,\n",
       " 'played': 1155,\n",
       " 'halifax': 1,\n",
       " 'investigating': 27,\n",
       " 'couple': 567,\n",
       " 'seemingly': 122,\n",
       " 'unconnected': 3,\n",
       " 'murders': 144,\n",
       " 'house': 693,\n",
       " 'main': 775,\n",
       " 'suspect': 123,\n",
       " 'script': 652,\n",
       " 'better': 1702,\n",
       " 'average': 224,\n",
       " 'production': 528,\n",
       " 'overall': 591,\n",
       " 'standard': 154,\n",
       " 'fails': 79,\n",
       " 'engage': 26,\n",
       " 'viewer': 464,\n",
       " 'particularly': 408,\n",
       " 'key': 159,\n",
       " 'momentsbr': 22,\n",
       " 'picked': 108,\n",
       " 'dvd': 910,\n",
       " 'mere': 72,\n",
       " 'last': 1123,\n",
       " 'week': 165,\n",
       " 'regular': 115,\n",
       " 'video': 464,\n",
       " 'store': 157,\n",
       " 'cannot': 348,\n",
       " 'begrudge': 1,\n",
       " 'expenditure': 1,\n",
       " 'acceptable': 29,\n",
       " 'price': 124,\n",
       " 'dont': 2397,\n",
       " 'fireworks': 14,\n",
       " 'another': 1503,\n",
       " 'aussie': 15,\n",
       " 'masterpiece': 316,\n",
       " 'delves': 12,\n",
       " 'world': 1632,\n",
       " 'unknown': 101,\n",
       " 'supernatural': 61,\n",
       " 'does': 2281,\n",
       " 'resort': 32,\n",
       " 'big': 1275,\n",
       " 'special': 726,\n",
       " 'effects': 566,\n",
       " 'overkill': 8,\n",
       " 'like': 6408,\n",
       " 'american': 844,\n",
       " 'flicks': 84,\n",
       " 'focuses': 91,\n",
       " 'emotional': 332,\n",
       " 'impact': 192,\n",
       " 'relatively': 78,\n",
       " 'simple': 531,\n",
       " 'rebecca': 9,\n",
       " 'co': 19,\n",
       " 'bring': 357,\n",
       " 'follows': 225,\n",
       " 'story': 4781,\n",
       " 'buy': 274,\n",
       " 'old': 1520,\n",
       " 'supposedly': 80,\n",
       " 'home': 754,\n",
       " 'woman': 971,\n",
       " 'never': 2288,\n",
       " 'went': 509,\n",
       " 'outside': 211,\n",
       " 'whose': 429,\n",
       " 'husband': 367,\n",
       " 'disappeared': 36,\n",
       " 'mysterious': 164,\n",
       " 'circumstances': 92,\n",
       " 'century': 226,\n",
       " 'ago': 445,\n",
       " 'strange': 324,\n",
       " 'things': 1309,\n",
       " 'begin': 224,\n",
       " 'happen': 313,\n",
       " 'adam': 118,\n",
       " 'begins': 356,\n",
       " 'turn': 457,\n",
       " 'actually': 1291,\n",
       " 'mass': 52,\n",
       " 'murderer': 74,\n",
       " 'highly': 628,\n",
       " 'recommended': 255,\n",
       " 'after': 2771,\n",
       " 'brief': 159,\n",
       " 'prologue': 9,\n",
       " 'showing': 313,\n",
       " 'masked': 15,\n",
       " 'stalking': 16,\n",
       " 'then': 2402,\n",
       " 'slashing': 4,\n",
       " 'throat': 25,\n",
       " 'older': 304,\n",
       " 'gentleman': 56,\n",
       " 'deserted': 23,\n",
       " 'urban': 103,\n",
       " 'australian': 66,\n",
       " 'street': 267,\n",
       " 'meet': 289,\n",
       " 'julie': 78,\n",
       " 'peter': 344,\n",
       " 'go': 1715,\n",
       " 'hunting': 70,\n",
       " 'manage': 74,\n",
       " 'loan': 8,\n",
       " 'fixerupper': 1,\n",
       " 'posh': 14,\n",
       " 'sydney': 22,\n",
       " 'turns': 469,\n",
       " 'physical': 139,\n",
       " 'disrepair': 1,\n",
       " 'problem': 382,\n",
       " 'may': 1470,\n",
       " 'hauntedbr': 1,\n",
       " 'combines': 34,\n",
       " 'memorable': 328,\n",
       " 'clichd': 29,\n",
       " 'direction': 442,\n",
       " 'catherine': 52,\n",
       " 'millar': 4,\n",
       " 'slightly': 217,\n",
       " 'above': 311,\n",
       " 'shockerbr': 1,\n",
       " 'biggest': 149,\n",
       " 'flaws': 147,\n",
       " 'seem': 755,\n",
       " 'partially': 25,\n",
       " 'due': 351,\n",
       " 'budget': 366,\n",
       " 'wholly': 33,\n",
       " 'excusable': 2,\n",
       " 'hurdle': 1,\n",
       " 'crucial': 42,\n",
       " 'occurs': 51,\n",
       " 'features': 273,\n",
       " 'wonky': 1,\n",
       " 'editing': 175,\n",
       " 'freeze': 11,\n",
       " 'frames': 22,\n",
       " 'series': 1553,\n",
       " 'stills': 10,\n",
       " 'used': 680,\n",
       " 'cover': 128,\n",
       " 'fact': 1220,\n",
       " 'theres': 865,\n",
       " 'suspense': 235,\n",
       " 'should': 1557,\n",
       " 'created': 223,\n",
       " 'staging': 14,\n",
       " 'fancy': 37,\n",
       " 'fix': 30,\n",
       " 'mix': 141,\n",
       " 'techniques': 58,\n",
       " 'great': 4592,\n",
       " 'atmosphere': 311,\n",
       " 'location': 114,\n",
       " 'lighting': 101,\n",
       " 'fog': 29,\n",
       " 'camera': 452,\n",
       " 'slowly': 189,\n",
       " 'following': 223,\n",
       " 'killer': 319,\n",
       " 'victim': 128,\n",
       " 'cutting': 60,\n",
       " 'back': 1837,\n",
       " 'forth': 60,\n",
       " 'track': 135,\n",
       " 'increasing': 15,\n",
       " 'proximity': 5,\n",
       " 'tracking': 35,\n",
       " 'cuts': 80,\n",
       " 'need': 619,\n",
       " 'slow': 310,\n",
       " 'attack': 132,\n",
       " 'needed': 198,\n",
       " 'longer': 179,\n",
       " 'clearer': 10,\n",
       " 'blocked': 3,\n",
       " 'stands': 174,\n",
       " 'strong': 530,\n",
       " 'television': 366,\n",
       " 'feel': 1098,\n",
       " 'low': 289,\n",
       " 'thatbr': 62,\n",
       " 'move': 245,\n",
       " 'present': 267,\n",
       " 'flow': 57,\n",
       " 'greatly': 76,\n",
       " 'improves': 17,\n",
       " 'lot': 1477,\n",
       " 'similarities': 52,\n",
       " 'forces': 121,\n",
       " 'subtler': 5,\n",
       " 'approach': 170,\n",
       " 'scriptwriter': 13,\n",
       " 'tony': 152,\n",
       " 'morphett': 2,\n",
       " 'effectively': 94,\n",
       " 'create': 219,\n",
       " 'slyly': 4,\n",
       " 'creepy': 160,\n",
       " 'scenarios': 15,\n",
       " 'often': 690,\n",
       " 'dramatic': 285,\n",
       " 'nature': 327,\n",
       " 'instead': 520,\n",
       " 'effectsoriented': 1,\n",
       " 'arrives': 77,\n",
       " 'take': 1213,\n",
       " 'away': 893,\n",
       " 'slabs': 2,\n",
       " 'iron': 28,\n",
       " 'bizarrely': 6,\n",
       " 'affixed': 2,\n",
       " 'interior': 21,\n",
       " 'wallbr': 3,\n",
       " 'fans': 545,\n",
       " 'section': 80,\n",
       " 'little': 2382,\n",
       " 'heavy': 155,\n",
       " 'realist': 5,\n",
       " 'drama': 598,\n",
       " 'least': 796,\n",
       " 'half': 463,\n",
       " 'hour': 267,\n",
       " 'primarily': 43,\n",
       " 'trying': 719,\n",
       " 'arrange': 12,\n",
       " 'financing': 4,\n",
       " 'settle': 36,\n",
       " 'writes': 31,\n",
       " 'fine': 623,\n",
       " 'intelligent': 225,\n",
       " 'dialogue': 444,\n",
       " 'material': 222,\n",
       " 'enough': 1071,\n",
       " 'suspenseful': 84,\n",
       " 'traditional': 124,\n",
       " 'aspects': 165,\n",
       " 'arise': 16,\n",
       " 'laterespecially': 1,\n",
       " 'youve': 253,\n",
       " 'gone': 265,\n",
       " 'similar': 371,\n",
       " 'travails': 3,\n",
       " 'while': 2115,\n",
       " 'own': 1490,\n",
       " 'housebr': 14,\n",
       " 'once': 911,\n",
       " 'settled': 27,\n",
       " 'weirder': 5,\n",
       " 'leave': 412,\n",
       " 'desired': 32,\n",
       " 'ideas': 178,\n",
       " 'performances': 894,\n",
       " 'help': 678,\n",
       " 'tension': 186,\n",
       " 'abundance': 16,\n",
       " 'death': 727,\n",
       " 'destruction': 53,\n",
       " 'filmtheres': 1,\n",
       " 'repair': 10,\n",
       " 'nightmares': 39,\n",
       " 'neither': 143,\n",
       " 'menace': 35,\n",
       " 'really': 4025,\n",
       " 'pointbr': 24,\n",
       " 'point': 949,\n",
       " 'human': 685,\n",
       " 'relationships': 178,\n",
       " 'number': 410,\n",
       " 'arcs': 11,\n",
       " 'exists': 62,\n",
       " 'metaphor': 34,\n",
       " 'catalyst': 9,\n",
       " 'stress': 35,\n",
       " 'romantic': 400,\n",
       " 'relationship': 447,\n",
       " 'make': 2328,\n",
       " 'sour': 17,\n",
       " 'possibly': 211,\n",
       " 'destroy': 66,\n",
       " 'neighborhood': 59,\n",
       " 'between': 1509,\n",
       " 'successful': 229,\n",
       " 'yuppies': 3,\n",
       " 'shows': 1145,\n",
       " 'these': 1977,\n",
       " 'problems': 345,\n",
       " 'afflict': 2,\n",
       " 'those': 1883,\n",
       " 'place': 893,\n",
       " 'blame': 49,\n",
       " 'external': 16,\n",
       " 'woe': 4,\n",
       " 'money': 470,\n",
       " 'health': 64,\n",
       " 'peters': 84,\n",
       " 'evolves': 16,\n",
       " 'striving': 7,\n",
       " 'corporate': 46,\n",
       " 'employee': 25,\n",
       " 'normal': 201,\n",
       " 'workbased': 1,\n",
       " 'friendships': 26,\n",
       " 'someone': 589,\n",
       " 'desperation': 45,\n",
       " 'becomes': 526,\n",
       " 'subversive': 12,\n",
       " 'scheming': 18,\n",
       " 'attain': 12,\n",
       " 'something': 1520,\n",
       " 'liberating': 8,\n",
       " 'meaningful': 56,\n",
       " 'learn': 293,\n",
       " 'shallow': 53,\n",
       " 'professional': 142,\n",
       " 'goes': 869,\n",
       " 'almost': 1122,\n",
       " 'literal': 30,\n",
       " 'nervous': 35,\n",
       " 'breakdown': 19,\n",
       " 'finally': 643,\n",
       " 'finds': 422,\n",
       " 'liberation': 10,\n",
       " 'liberates': 1,\n",
       " 'herself': 354,\n",
       " 'failing': 30,\n",
       " 'relationshipbr': 9,\n",
       " 'quite': 1523,\n",
       " 'transcends': 20,\n",
       " 'madefortelevision': 5,\n",
       " 'clunkiness': 2,\n",
       " 'tv': 962,\n",
       " 'admirable': 27,\n",
       " 'ambitions': 22,\n",
       " 'anyone': 879,\n",
       " 'fond': 42,\n",
       " 'haunted': 71,\n",
       " 'psycho': 81,\n",
       " 'horrorthrillers': 3,\n",
       " 'bit': 1269,\n",
       " 'metaphorical': 7,\n",
       " 'depth': 191,\n",
       " 'plenty': 257,\n",
       " 'enjoy': 837,\n",
       " 'certainly': 607,\n",
       " 'worth': 890,\n",
       " 'spending': 24,\n",
       " 'local': 319,\n",
       " 'pbs': 9,\n",
       " 'station': 117,\n",
       " 'asking': 67,\n",
       " 'copy': 231,\n",
       " 'showed': 200,\n",
       " 'factoring': 1,\n",
       " 'shipping': 3,\n",
       " 'handling': 29,\n",
       " 'rental': 43,\n",
       " 'definitely': 735,\n",
       " 'watching': 1422,\n",
       " 'free': 185,\n",
       " 'verhoevens': 27,\n",
       " 'utter': 41,\n",
       " 'complete': 291,\n",
       " 'garbage': 53,\n",
       " 'hes': 996,\n",
       " 'disgusting': 40,\n",
       " 'hack': 13,\n",
       " 'director': 1276,\n",
       " 'ashamed': 22,\n",
       " 'admission': 20,\n",
       " 'read': 638,\n",
       " 'chapters': 18,\n",
       " 'book': 753,\n",
       " 'got': 1182,\n",
       " 'bored': 102,\n",
       " 'decided': 200,\n",
       " 'scratchbr': 1,\n",
       " 'heinlein': 2,\n",
       " 'supported': 34,\n",
       " 'trash': 80,\n",
       " 'hed': 43,\n",
       " 'alive': 149,\n",
       " 'basically': 258,\n",
       " 'steals': 119,\n",
       " 'name': 510,\n",
       " 'mocks': 5,\n",
       " 'politics': 90,\n",
       " 'portion': 33,\n",
       " 'throws': 55,\n",
       " 'ta': 10,\n",
       " 'idiot': 32,\n",
       " 'moviegoer': 9,\n",
       " 'boredbr': 1,\n",
       " 'anime': 106,\n",
       " 'perfect': 866,\n",
       " 'mostly': 344,\n",
       " 'accurate': 140,\n",
       " 'best': 3161,\n",
       " 'tell': 550,\n",
       " 'robert': 479,\n",
       " 'heinleins': 7,\n",
       " 'novel': 359,\n",
       " 'starship': 20,\n",
       " 'troopers': 17,\n",
       " 'messed': 22,\n",
       " 'around': 1195,\n",
       " 'recent': 206,\n",
       " 'everything': 787,\n",
       " 'paul': 460,\n",
       " 'games': 134,\n",
       " 'none': 246,\n",
       " 'speak': 162,\n",
       " 'captured': 124,\n",
       " 'spirit': 203,\n",
       " 'usually': 335,\n",
       " 'unrelated': 19,\n",
       " 'spin': 49,\n",
       " 'off': 1662,\n",
       " 'less': 669,\n",
       " 'know': 2035,\n",
       " 'japan': 117,\n",
       " 'animated': 254,\n",
       " 'adaptation': 182,\n",
       " 'already': 439,\n",
       " 'year': 815,\n",
       " 'despite': 516,\n",
       " 'differences': 59,\n",
       " 'part': 1519,\n",
       " 'plotwise': 10,\n",
       " 'faithful': 92,\n",
       " 'classicbr': 16,\n",
       " 'obvious': 283,\n",
       " 'plus': 204,\n",
       " 'presence': 158,\n",
       " 'powered': 10,\n",
       " 'armor': 18,\n",
       " 'exoskeletons': 1,\n",
       " 'deprived': 5,\n",
       " 'characters': 2572,\n",
       " 'space': 224,\n",
       " 'travel': 109,\n",
       " 'fair': 140,\n",
       " 'amount': 173,\n",
       " 'each': 1194,\n",
       " 'events': 376,\n",
       " 'differently': 28,\n",
       " 'books': 205,\n",
       " 'rico': 8,\n",
       " 'carmen': 40,\n",
       " 'entanglement': 1,\n",
       " 'touched': 87,\n",
       " 'upon': 305,\n",
       " 'interaction': 40,\n",
       " 'inferior': 31,\n",
       " 'gets': 1058,\n",
       " 'treatment': 89,\n",
       " 'superior': 148,\n",
       " 'political': 283,\n",
       " 'views': 76,\n",
       " 'merely': 116,\n",
       " 'excised': 2,\n",
       " 'opposed': 46,\n",
       " 'reversed': 12,\n",
       " 'payoff': 15,\n",
       " 'climatic': 13,\n",
       " 'battle': 295,\n",
       " 'klendathu': 1,\n",
       " 'bugsaliens': 1,\n",
       " 'kind': 895,\n",
       " 'suits': 47,\n",
       " 'seen': 2430,\n",
       " 'versionbr': 17,\n",
       " 'enjoyed': 645,\n",
       " 'because': 2944,\n",
       " 'wanted': 411,\n",
       " 'vision': 143,\n",
       " 'look': 1354,\n",
       " 'ok': 207,\n",
       " 'lets': 293,\n",
       " 'clear': 270,\n",
       " 'scifi': 238,\n",
       " 'reason': 556,\n",
       " 'love': 3097,\n",
       " 'stargate': 68,\n",
       " 'sg': 54,\n",
       " 'jack': 370,\n",
       " 'oneil': 5,\n",
       " 'takes': 910,\n",
       " 'team': 354,\n",
       " 'round': 75,\n",
       " 'device': 61,\n",
       " 'creates': 122,\n",
       " 'wormhole': 2,\n",
       " 'gives': 740,\n",
       " 'ability': 168,\n",
       " 'distant': 58,\n",
       " 'worlds': 138,\n",
       " 'sound': 386,\n",
       " 'usual': 385,\n",
       " 'scifiseries': 2,\n",
       " 'set': 847,\n",
       " 'today': 506,\n",
       " 'millennium': 11,\n",
       " 'happenings': 17,\n",
       " 'relate': 134,\n",
       " 'jump': 83,\n",
       " 'any': 2231,\n",
       " 'terms': 182,\n",
       " 'names': 124,\n",
       " 'gadgets': 10,\n",
       " 'course': 948,\n",
       " 'thanks': 216,\n",
       " 'likes': 178,\n",
       " 'keep': 628,\n",
       " 'terminology': 5,\n",
       " 'nice': 803,\n",
       " 'blending': 18,\n",
       " 'humor': 545,\n",
       " 'loads': 39,\n",
       " 'youre': 588,\n",
       " 'going': 1275,\n",
       " 'use': 638,\n",
       " 'bad': 1289,\n",
       " 'ones': 475,\n",
       " 'excellent': 1198,\n",
       " 'program': 80,\n",
       " 'season': 425,\n",
       " 'since': 1110,\n",
       " 'episodes': 505,\n",
       " 'keeps': 301,\n",
       " 'getting': 524,\n",
       " 'seasons': 124,\n",
       " 'now': 1690,\n",
       " 'richard': 423,\n",
       " 'dean': 73,\n",
       " 'anderson': 74,\n",
       " 'addition': 150,\n",
       " 'ben': 250,\n",
       " 'browder': 6,\n",
       " 'claudie': 1,\n",
       " 'black': 687,\n",
       " 'still': 2418,\n",
       " 'given': 657,\n",
       " 'show': 2429,\n",
       " 'strength': 119,\n",
       " 'original': 1038,\n",
       " 'sadly': 152,\n",
       " 'channel': 106,\n",
       " 'rid': 50,\n",
       " 'amazing': 772,\n",
       " 'hope': 587,\n",
       " 'relay': 2,\n",
       " 'making': 828,\n",
       " 'direct': 127,\n",
       " 'hopefully': 77,\n",
       " 'atlantis': 59,\n",
       " 'th': 321,\n",
       " 'third': 284,\n",
       " 'works': 622,\n",
       " 'franchise': 45,\n",
       " 'nowhere': 79,\n",
       " 'near': 266,\n",
       " 'dead': 519,\n",
       " 'must': 1155,\n",
       " 'people': 3163,\n",
       " 'genres': 33,\n",
       " 'wide': 103,\n",
       " 'range': 97,\n",
       " 'ages': 130,\n",
       " 'types': 84,\n",
       " 'watch': 2430,\n",
       " 'cheesy': 149,\n",
       " 'seriesbr': 32,\n",
       " 'escaping': 24,\n",
       " 'facts': 80,\n",
       " 'try': 549,\n",
       " 'excuse': 54,\n",
       " 'yourself': 285,\n",
       " 'explain': 131,\n",
       " 'remains': 193,\n",
       " 'borrow': 16,\n",
       " 'steal': 78,\n",
       " 'briskly': 4,\n",
       " 'fx': 31,\n",
       " 'arent': 261,\n",
       " 'nearly': 299,\n",
       " 'impressive': 223,\n",
       " 'blow': 70,\n",
       " 'chair': 49,\n",
       " 'couch': 21,\n",
       " 'matter': 413,\n",
       " 'eitherbr': 24,\n",
       " 'deserves': 239,\n",
       " 'credit': 169,\n",
       " 'every': 1427,\n",
       " 'stolen': 65,\n",
       " 'idea': 511,\n",
       " 'count': 101,\n",
       " 'thats': 1072,\n",
       " 'worse': 151,\n",
       " 'episodebr': 15,\n",
       " 'probably': 1040,\n",
       " 'minutes': 586,\n",
       " 'long': 1148,\n",
       " 'moviesbr': 42,\n",
       " 'being': 2357,\n",
       " 'able': 517,\n",
       " 'quality': 424,\n",
       " 'delivering': 28,\n",
       " 'pushing': 63,\n",
       " 'storyline': 265,\n",
       " 'further': 214,\n",
       " 'makes': 1761,\n",
       " 'specialbr': 4,\n",
       " 'am': 955,\n",
       " 'selections': 8,\n",
       " 'follow': 290,\n",
       " 'perhaps': 642,\n",
       " 'hold': 189,\n",
       " 'standards': 125,\n",
       " 'surprised': 350,\n",
       " 'found': 964,\n",
       " 'myself': 370,\n",
       " 'caughtbr': 1,\n",
       " 'decide': 140,\n",
       " 'brush': 14,\n",
       " 'yet': 1151,\n",
       " 'tacky': 14,\n",
       " 'stick': 104,\n",
       " 'youll': 485,\n",
       " 'talking': 262,\n",
       " 'scott': 229,\n",
       " 'bartletts': 2,\n",
       " 'offon': 4,\n",
       " 'nine': 66,\n",
       " 'pure': 236,\n",
       " 'craziness': 13,\n",
       " 'fullfrontal': 6,\n",
       " 'assault': 24,\n",
       " 'psychedelic': 16,\n",
       " 'pulsating': 6,\n",
       " 'epilepsyinducing': 1,\n",
       " 'flashing': 14,\n",
       " 'lights': 79,\n",
       " 'colours': 32,\n",
       " 'merging': 7,\n",
       " 'avantegarde': 1,\n",
       " 'cinema': 598,\n",
       " 'bartlett': 2,\n",
       " 'uses': 242,\n",
       " 'images': 228,\n",
       " 'face': 595,\n",
       " 'form': 318,\n",
       " 'provoke': 12,\n",
       " 'sequence': 340,\n",
       " 'reactions': 55,\n",
       " 'integrating': 4,\n",
       " 'biological': 11,\n",
       " 'phenomena': 6,\n",
       " 'highlyindustrial': 1,\n",
       " 'modern': 391,\n",
       " 'technology': 105,\n",
       " 'sense': 754,\n",
       " 'represents': 55,\n",
       " 'humanity': 124,\n",
       " 'tools': 14,\n",
       " 'machinery': 13,\n",
       " 'theme': 325,\n",
       " 'connects': 20,\n",
       " 'loosely': 35,\n",
       " 'subplot': 59,\n",
       " 'hal': 59,\n",
       " 'stanley': 93,\n",
       " 'kubricks': 32,\n",
       " 'odyssey': 38,\n",
       " 'indeed': 298,\n",
       " 'before': 1521,\n",
       " 'closeup': 32,\n",
       " 'eye': 250,\n",
       " 'recalls': 16,\n",
       " 'dave': 43,\n",
       " 'bowmans': 1,\n",
       " 'journey': 259,\n",
       " 'visuals': 111,\n",
       " 'richlycoloured': 1,\n",
       " 'confronting': 10,\n",
       " 'blend': 64,\n",
       " 'sharp': 86,\n",
       " 'vivid': 64,\n",
       " 'increasinglygrainy': 1,\n",
       " 'sitting': 121,\n",
       " 'close': 408,\n",
       " 'screen': 836,\n",
       " 'end': 1905,\n",
       " 'product': 68,\n",
       " 'recorded': 44,\n",
       " 'monitorbr': 1,\n",
       " 'appears': 286,\n",
       " 'confusion': 53,\n",
       " 'release': 348,\n",
       " 'date': 144,\n",
       " 'imdb': 170,\n",
       " 'lists': 15,\n",
       " 'both': 1647,\n",
       " 'national': 95,\n",
       " 'registry': 1,\n",
       " 'preservation': 7,\n",
       " 'foundation': 23,\n",
       " 'correct': 72,\n",
       " 'disparity': 5,\n",
       " 'reflects': 42,\n",
       " 'completion': 4,\n",
       " 'public': 206,\n",
       " 'screening': 67,\n",
       " 'way': 2840,\n",
       " 'distinctly': 19,\n",
       " 'ahead': 154,\n",
       " 'occasionally': 104,\n",
       " 'reminiscent': 79,\n",
       " 's': 1598,\n",
       " 'music': 1249,\n",
       " 'brisk': 13,\n",
       " 'techno': 4,\n",
       " 'wouldnt': 271,\n",
       " 'amiss': 1,\n",
       " 'captures': 148,\n",
       " 'grainy': 19,\n",
       " 'fragmented': 7,\n",
       " 'presenting': 28,\n",
       " 'warped': 13,\n",
       " 'perspective': 118,\n",
       " 'processing': 4,\n",
       " 'information': 150,\n",
       " 'thought': 1136,\n",
       " 'please': 255,\n",
       " 'laugh': 415,\n",
       " 'freethinking': 1,\n",
       " 'interpretation': 85,\n",
       " 'extraterrestrial': 4,\n",
       " 'civilisation': 3,\n",
       " 'capturing': 41,\n",
       " 'earths': 9,\n",
       " 'signals': 5,\n",
       " 'receive': 48,\n",
       " 'disjointed': 14,\n",
       " 'alien': 93,\n",
       " 'documentation': 7,\n",
       " 'bizarre': 150,\n",
       " 'montage': 44,\n",
       " 'vaguelyfamiliar': 1,\n",
       " 'imagery': 74,\n",
       " 'couldnt': 346,\n",
       " 'coherent': 17,\n",
       " 'mankind': 31,\n",
       " 'eventually': 283,\n",
       " 'heading': 20,\n",
       " 'towards': 254,\n",
       " 'irreversible': 5,\n",
       " 'purity': 12,\n",
       " 'artificiality': 4,\n",
       " 'showbr': 42,\n",
       " 'repeat': 49,\n",
       " 'huge': 317,\n",
       " ...}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_train_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While developing the model, this mathematical model was used:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_i = \\frac{x_i + \\alpha}{N + \\alpha d} \\qquad (i = 1, ..., d)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the logic is quite simple. To predict a review as positive, it must have a greater probability of being positive than being negative. That's what the Naive Bayes does. In my implementation ngram and alpha are adjustable for experimenting purposes. \n",
    "\n",
    "fit() function fits the data as BoW to the model. Actually this was not my original idea but since you asked us to prepare BoWs before the model I came up with this format.\n",
    "\n",
    "predict() function does the prediction for all reviews and returns predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, ngram: int=1, alpha: int=1):\n",
    "        self.ngram = ngram\n",
    "        self.pos_dict = defaultdict(int)\n",
    "        self.neg_dict = defaultdict(int)\n",
    "        self.pos_total_count = 0\n",
    "        self.neg_total_count = 0\n",
    "        self.vocab = set()\n",
    "        self.predictions = []\n",
    "        self.alpha = alpha  \n",
    "    \n",
    "    def fit(self, pos_dict: dict[str, int], neg_dict: dict[str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Trains the model with the positive and negative BoW dictionaries.\n",
    "\n",
    "        :param pos_dict: Dictionary for positive sentiment n-grams\n",
    "        :param neg_dict: Dictionary for negative sentiment n-grams\n",
    "        \"\"\"\n",
    "        self.pos_dict = pos_dict\n",
    "        self.neg_dict = neg_dict\n",
    "        \n",
    "        self.pos_total_count = sum(self.pos_dict.values())\n",
    "        self.neg_total_count = sum(self.neg_dict.values())\n",
    "        self.vocab = set(self.pos_dict.keys()).union(set(self.neg_dict.keys()))\n",
    "    \n",
    "    def predict(self, reviews: pd.DataFrame) -> list[str]:\n",
    "        \"\"\"\n",
    "        Predicts sentiment for reviews.\n",
    "\n",
    "        :param reviews: Reviews\n",
    "        :return: Predictions\n",
    "        \"\"\"\n",
    "        self.predictions = []\n",
    "\n",
    "        for _, row  in reviews.iterrows():\n",
    "            tokens = tokenize(row['review'])\n",
    "            ngrams = zip(*[tokens[i:] for i in range(self.ngram)])\n",
    "\n",
    "            pos_log_prob = 0.0\n",
    "            neg_log_prob = 0.0\n",
    "\n",
    "            # variables for Laplace (additive) smoothing\n",
    "            alpha = self.alpha\n",
    "            v_size = len(self.vocab)  \n",
    "\n",
    "            for ngram_tuple in ngrams:\n",
    "                ngram_str = ' '.join(ngram_tuple)\n",
    "\n",
    "                # Laplace smoothing\n",
    "                pos_prob = math.log((self.pos_dict.get(ngram_str, 0) + alpha) / (self.pos_total_count + alpha * v_size))\n",
    "                pos_log_prob += pos_prob # taking advantage of the log properties\n",
    "\n",
    "                # Laplace smoothing again\n",
    "                neg_prob = math.log((self.neg_dict.get(ngram_str, 0) + alpha) / (self.neg_total_count + alpha * v_size))\n",
    "                neg_log_prob += neg_prob # taking advantage of the log properties again\n",
    "\n",
    "            self.predictions.append('pos' if pos_log_prob > neg_log_prob else 'neg')\n",
    "    \n",
    "        return self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true: list[str], y_pred: list[str]) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model using accuracy, precision, recall and F1 score.\n",
    "\n",
    "    :param y_true: True labels\n",
    "    :param y_pred: Predicted labels\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, pos_label='pos')\n",
    "    recall = recall_score(y_true, y_pred, pos_label='pos')\n",
    "    f1 = f1_score(y_true, y_pred, pos_label='pos')\n",
    "    \n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, testing unigram and bigram that we have already prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8139\n",
      "Precision: 0.8765\n",
      "Recall: 0.7307\n",
      "F1 Score: 0.7970\n"
     ]
    }
   ],
   "source": [
    "# unigram\n",
    "classifier = NaiveBayesClassifier(ngram=1)\n",
    "classifier.fit(unigram_train_pos, unigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8362\n",
      "Precision: 0.8404\n",
      "Recall: 0.8302\n",
      "F1 Score: 0.8352\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "classifier = NaiveBayesClassifier(ngram=2)\n",
    "classifier.fit(bigram_train_pos, bigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for experimental purposes, I want to do also 3, 4, and 5-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_train_pos, trigram_train_neg = bow(train_data, ngram=3)\n",
    "quadgram_train_pos, quadgram_train_neg = bow(train_data, ngram=4)\n",
    "pentagram_train_pos, pentagram_train_neg = bow(train_data, ngram=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5904\n",
      "Precision: 0.5501\n",
      "Recall: 0.9930\n",
      "F1 Score: 0.7080\n"
     ]
    }
   ],
   "source": [
    "# trigram\n",
    "classifier = NaiveBayesClassifier(ngram=3)\n",
    "classifier.fit(trigram_train_pos, trigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5142\n",
      "Precision: 0.5072\n",
      "Recall: 0.9996\n",
      "F1 Score: 0.6729\n"
     ]
    }
   ],
   "source": [
    "# quadgram\n",
    "classifier = NaiveBayesClassifier(ngram=4)\n",
    "classifier.fit(quadgram_train_pos, quadgram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5061\n",
      "Precision: 0.5031\n",
      "Recall: 1.0000\n",
      "F1 Score: 0.6694\n"
     ]
    }
   ],
   "source": [
    "# pentagram\n",
    "classifier = NaiveBayesClassifier(ngram=5)\n",
    "classifier.fit(pentagram_train_pos, pentagram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are quite varying. Now I want to test the effect of the alpha value, so I should choose one gram to move on. Judging from the F1 Scores, bigram looks the best to me. For alpha values, I will be trying 0.5, 2, and 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8312\n",
      "Precision: 0.8065\n",
      "Recall: 0.8715\n",
      "F1 Score: 0.8377\n"
     ]
    }
   ],
   "source": [
    "# alpha = 0.5\n",
    "classifier = NaiveBayesClassifier(ngram=2, alpha=0.5)\n",
    "classifier.fit(bigram_train_pos, bigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8255\n",
      "Precision: 0.8816\n",
      "Recall: 0.7520\n",
      "F1 Score: 0.8117\n"
     ]
    }
   ],
   "source": [
    "# alpha = 2\n",
    "classifier = NaiveBayesClassifier(ngram=2, alpha=2)\n",
    "classifier.fit(bigram_train_pos, bigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7773\n",
      "Precision: 0.9226\n",
      "Recall: 0.6054\n",
      "F1 Score: 0.7311\n"
     ]
    }
   ],
   "source": [
    "# alpha = 5\n",
    "classifier = NaiveBayesClassifier(ngram=2, alpha=5)\n",
    "classifier.fit(bigram_train_pos, bigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like a smaller alpha value gave a better result. This might be caused because the effect of smoothing being not really needed for this data and implementation. For testing purposes I will be trying an even lower alpha value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8049\n",
      "Precision: 0.8156\n",
      "Recall: 0.7880\n",
      "F1 Score: 0.8016\n"
     ]
    }
   ],
   "source": [
    "# alpha = 0.01\n",
    "classifier = NaiveBayesClassifier(ngram=2, alpha=0.01)\n",
    "classifier.fit(bigram_train_pos, bigram_train_neg)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, the best overall performing model is bigram with alpha=1 in my opinion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gensim\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = []\n",
    "\n",
    "for _, row in train_data.iterrows():\n",
    "    tokens = tokenize(row['review'])\n",
    "    train_tokens.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_embedding(review: str, word2vec_model: Word2Vec) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Function to get the review embedding.\n",
    "\n",
    "    :param review: A review\n",
    "    :param word2vec_model: A Word2Vec model\n",
    "    :return: The review embedding\n",
    "    \"\"\"\n",
    "    tokens = tokenize(review)\n",
    "    embeddings = [word2vec_model.wv[word] for word in tokens if word in word2vec_model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_data['sentiment']\n",
    "y_test = test_data['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7995\n",
      "Precision: 0.8315\n",
      "Recall: 0.7513\n",
      "F1 Score: 0.7894\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with the parameters a bit. Let's try a larger vector size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=5, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8023\n",
      "Precision: 0.8334\n",
      "Recall: 0.7557\n",
      "F1 Score: 0.7926\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, we get slightly better results. Now let's change the window parameter. From what I learned, this window parameter is a range value for the distance between the words for each word. Let's decrease it a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=3, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7962\n",
      "Precision: 0.8300\n",
      "Recall: 0.7450\n",
      "F1 Score: 0.7852\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should increase it a bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=7, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8026\n",
      "Precision: 0.8327\n",
      "Recall: 0.7574\n",
      "F1 Score: 0.7933\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, let's try an extreme value just for kicks, maybe something unexpected will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=12, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8103\n",
      "Precision: 0.8402\n",
      "Recall: 0.7662\n",
      "F1 Score: 0.8015\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixed comments :) Now let's use a bigger variety of parameters. Window size 5 will be chosen from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=train_tokens, vector_size=300, window=5, min_count=3, workers=8, sg=1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8103\n",
      "Precision: 0.8402\n",
      "Recall: 0.7662\n",
      "F1 Score: 0.8015\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we got the best performance from the most unexpected model (the one I used an \"extreme\" window value). So, let's just squeeze it really good :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=20, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8087\n",
      "Precision: 0.8379\n",
      "Recall: 0.7656\n",
      "F1 Score: 0.8001\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like it just keeps getting better as the window increases. I can think of only one explanation for this matter: Reviews are generally quite lengthy and the sentiments may be spanned on multiple words, causing a long window. So, let's test it with an even greater value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(sentences=train_tokens, vector_size=300, window=50, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_review_embedding(review, word2vec) for review in train_data['review']])\n",
    "X_test = np.array([get_review_embedding(review, word2vec) for review in test_data['review']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8143\n",
      "Precision: 0.8431\n",
      "Recall: 0.7724\n",
      "F1 Score: 0.8062\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "evaluate(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm no expert but I see this as an absolute win!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's use NLTK and see what's the deal with this magical toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Bora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now some small changes must be made on the tokenizer, BoW and Classifier to use them with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_nltk(text: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Tokenizes text using NLTK, converts to lowercase, removes non-alphabetic characters,\n",
    "    and removes stop words.\n",
    "\n",
    "    :param text: A text string to be tokenized\n",
    "    :return: A list of tokens\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # NLTK tokenizer\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in nltk_stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_nltk(data: pd.DataFrame, ngram: int=1) -> tuple[dict[str, int], dict[str, int]]:\n",
    "    \"\"\"\n",
    "    Function to create a bag of words from a list of reviews using NLTK.\n",
    "\n",
    "    :param data: A pandas DataFrame with the reviews and the sentiment\n",
    "    :param ngram: The n-gram order (default=1)\n",
    "    :return: A tuple with the positive and negative dictionaries\n",
    "    \"\"\"\n",
    "    pos_dict = defaultdict(int)\n",
    "    neg_dict = defaultdict(int)\n",
    "    \n",
    "    # over reviews and sentiments\n",
    "    for _, row in data.iterrows():\n",
    "        tokens = tokenize_nltk(row['review']) # using NLTK tokenizer\n",
    "        sentiment = row['sentiment']\n",
    "        \n",
    "        ngram_list = ngrams(tokens, ngram)\n",
    "\n",
    "        for ngram_tuple in ngram_list:\n",
    "            ngram_str = ' '.join(ngram_tuple) # join tuple to form the n-gram string\n",
    "            if sentiment == 'pos':\n",
    "                pos_dict[ngram_str] += 1\n",
    "            else:\n",
    "                neg_dict[ngram_str] += 1\n",
    "\n",
    "    return dict(pos_dict), dict(neg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifierNLTK:\n",
    "    def __init__(self, ngram: int=1, alpha: int=1):\n",
    "        self.ngram = ngram\n",
    "        self.pos_dict = defaultdict(int)\n",
    "        self.neg_dict = defaultdict(int)\n",
    "        self.pos_total_count = 0\n",
    "        self.neg_total_count = 0\n",
    "        self.vocab = set()\n",
    "        self.predictions = []\n",
    "        self.alpha = alpha  \n",
    "    \n",
    "    def fit(self, pos_dict: dict[str, int], neg_dict: dict[str, int]):\n",
    "        \"\"\"\n",
    "        Trains the model with the positive and negative BoW dictionaries.\n",
    "\n",
    "        :param pos_dict: Dictionary for positive sentiment n-grams\n",
    "        :param neg_dict: Dictionary for negative sentiment n-grams\n",
    "        \"\"\"\n",
    "        self.pos_dict = pos_dict\n",
    "        self.neg_dict = neg_dict\n",
    "        \n",
    "        self.pos_total_count = sum(self.pos_dict.values())\n",
    "        self.neg_total_count = sum(self.neg_dict.values())\n",
    "        self.vocab = set(self.pos_dict.keys()).union(set(self.neg_dict.keys()))\n",
    "    \n",
    "    def predict(self, reviews: pd.DataFrame) -> list[str]:\n",
    "        \"\"\"\n",
    "        Predicts sentiment for reviews.\n",
    "\n",
    "        :param reviews: Reviews\n",
    "        :return: Predictions\n",
    "        \"\"\"\n",
    "        self.predictions = []  \n",
    "        \n",
    "        for _, row  in reviews.iterrows():\n",
    "            tokens = tokenize(row['review'])\n",
    "            ngrams = zip(*[tokens[i:] for i in range(self.ngram)])\n",
    "\n",
    "            pos_log_prob = 0.0\n",
    "            neg_log_prob = 0.0\n",
    "\n",
    "            # variables for Laplace (additive) smoothing\n",
    "            alpha = self.alpha\n",
    "            v_size = len(self.vocab)  \n",
    "\n",
    "            for ngram_tuple in ngrams:\n",
    "                ngram_str = ' '.join(ngram_tuple)\n",
    "\n",
    "                # Laplace smoothing\n",
    "                pos_prob = math.log((self.pos_dict.get(ngram_str, 0) + alpha) / (self.pos_total_count + alpha * v_size))\n",
    "                pos_log_prob += pos_prob # taking advantage of the log properties\n",
    "\n",
    "                # Laplace smoothing again\n",
    "                neg_prob = math.log((self.neg_dict.get(ngram_str, 0) + alpha) / (self.neg_total_count + alpha * v_size))\n",
    "                neg_log_prob += neg_prob # taking advantage of the log properties again\n",
    "\n",
    "            self.predictions.append('pos' if pos_log_prob > neg_log_prob else 'neg')\n",
    "    \n",
    "        return self.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_train_pos_nltk, unigram_train_neg_nltk = bow_nltk(train_data, ngram=1)\n",
    "bigram_train_pos_nltk, bigram_train_neg_nltk = bow_nltk(train_data, ngram=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how good is that NLTK, shall we?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8238\n",
      "Precision: 0.8264\n",
      "Recall: 0.8198\n",
      "F1 Score: 0.8231\n"
     ]
    }
   ],
   "source": [
    "# unigram\n",
    "classifier = NaiveBayesClassifierNLTK(ngram=1)\n",
    "classifier.fit(unigram_train_pos_nltk, unigram_train_neg_nltk)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8056\n",
      "Precision: 0.7538\n",
      "Recall: 0.9076\n",
      "F1 Score: 0.8236\n"
     ]
    }
   ],
   "source": [
    "# bigram\n",
    "classifier = NaiveBayesClassifierNLTK(ngram=2)\n",
    "classifier.fit(bigram_train_pos_nltk, bigram_train_neg_nltk)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's try with a smaller alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8213\n",
      "Precision: 0.8224\n",
      "Recall: 0.8196\n",
      "F1 Score: 0.8210\n"
     ]
    }
   ],
   "source": [
    "# alpha = 0.5\n",
    "classifier = NaiveBayesClassifierNLTK(ngram=1, alpha=0.5)\n",
    "classifier.fit(unigram_train_pos_nltk, unigram_train_neg_nltk)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe, an even smaller one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8102\n",
      "Precision: 0.8197\n",
      "Recall: 0.7953\n",
      "F1 Score: 0.8073\n"
     ]
    }
   ],
   "source": [
    "# alpha = 0.1\n",
    "classifier = NaiveBayesClassifierNLTK(ngram=1, alpha=0.1)\n",
    "classifier.fit(unigram_train_pos_nltk, unigram_train_neg_nltk)\n",
    "predictions = classifier.predict(test_data)\n",
    "evaluate(test_data['sentiment'], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK maybe not :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing best scores of all approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|               |   From Scratch  |            Word2Vec + LogReg            |       NLTK       |\n",
    "|---------------|-----------------|-----------------------------------------|------------------|\n",
    "|   **Params**  | Bigram, alpha=1 | Vector Size=300, Window=50, Min Count=1 | Unigram, alpha=1 | \n",
    "|  **Accuracy** |      <span style=\"color: green;\">**0.8362**</span>     |                  0.8112                 |       0.8238     |\n",
    "| **Precision** |      0.8404     |                 <span style=\"color: green;\">**0.8406**</span>                 |       0.8264     |\n",
    "|   **Recall**  |      <span style=\"color: green;\">**0.8302**</span>     |                  0.7680                 |       0.8198     |\n",
    "|  **F1 Score** |      <span style=\"color: green;\">**0.8352**</span>     |                  0.8027                 |       0.8231     |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, my implementation from scratch looks like the best performing one. It may be caused by some limitations, actually, in my opinion. For example, I'm pretty sure that my stopwords are incredibly narrower than NLTK's. Actually when I was deciding on my stopwords I did a bit of research to find a pre-defined stopwords list that is hopefully closer to NLTK's. But inspecting that list further, I realized it consisted some words like \"good\", which actually could hold sentiment for movie reviews. So I just sampled it to a certain degree which felt enough, filtering some words that I thought to be meaningful. So I guess my stopwords are less biased, that might be the reason for these results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
