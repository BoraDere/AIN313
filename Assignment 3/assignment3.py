# -*- coding: utf-8 -*-
"""assignment3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16X3UBP4eCFAbJ0-R22aAnF9QZa88SmJW

# PART I: Theory Questions

---

## 1. Question

Activation functions are mathematical functions that provide non-linearity to the model. They are applied to the output of the each neuron, before passing it to the next layer. They are important because as I mentioned before, they help introducing non-linearity to the neural network. Another importance can be their ability to be differentiated. This makes the usage of backpropagation possible. Also some activation functions like Softmax can be used in the output layer of neural networks to provide probabilities for classifiacion tasks.

---

## 2. Question

The table is the answer:

|   Layer  | Output Volume Shape | Number of Parameters |
|----------|---------------------|----------------------|
|   Input  |     (64, 64, 3)     |          0           |
|  CONV5-8 |     (60, 60, 8)     |         608          |
|  POOL-2  |     (30, 30, 8)     |          0           |
| CONV3-16 |     (28, 28, 16)    |        1168          |
|  POOL-3  |     (13, 13, 16)    |          0           |
|   FC-30  |         (30,)       |        81150         |
|   FC-5   |         (5,)        |         155          |

---

# PART II: Classification of Skin Lesion Images using Neural Network

## 0- Imports and Constants
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset

import os
from PIL import Image
from tqdm import tqdm

DATA_DIR = '311PA3_melanoma_dataset'
torch.manual_seed(42)
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""---

## 1- Creating Custom Structures for Backend

### 1.1- Dataset
"""

class MelanomaDataset(Dataset):
    def __init__(self, data_dir, target_size=(300, 300), augmentations=None, transformations=None, train=True):
        self.data_dir = data_dir
        self.target_size = target_size
        self.augmentations = augmentations
        self.transforms = transformations
        self.train = train
        self.samples = self.load_samples_(self.train)

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]
        img = Image.open(img_path).convert('RGB')

        if self.train and self.augmentations:
            img = self.augmentations(img)

        if self.transforms:
            img = self.transforms(img)

        return img, 0 if label == 'benign' else 1

    def load_samples_(self, train=True):
        samples = []
        mode = 'train' if train else 'test'
        dir = os.path.join(self.data_dir, mode)

        for label in os.listdir(dir):
            label_dir = os.path.join(dir, label)
            for img in os.listdir(label_dir):
                samples.append((os.path.join(label_dir, img), label))

        return samples

"""---

### 1.2- Activation Function Map
"""

activation_func_map = {
    'relu': nn.ReLU,
    'sigmoid': nn.Sigmoid
}

"""---

### 1.3- Models

#### Multi-Layer Neural Network
"""

class MultiLayerNN(nn.Module):
    def __init__(self, input_size, activation_func):
        super(MultiLayerNN, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 1)
        self.activation_func = activation_func

    def forward(self, x):
        x = x.flatten(start_dim=1)
        x = self.fc1(x)
        x = activation_func_map[self.activation_func]()(x)
        x = self.fc2(x)
        x = activation_func_map[self.activation_func]()(x)
        x = self.fc3(x)
        x = torch.sigmoid(x)

        return x

"""---

#### Convolutional Neural Network
"""

class ConvNN(nn.Module):
    def __init__(self, activation_func):
        super(ConvNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 128, 5, padding=2)
        self.conv2 = nn.Conv2d(128, 256, 3, padding=1)
        self.fc1 = nn.Linear(256 * 75 * 75, 128)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(128, 1)
        self.activation_func = activation_func

    def forward(self, x):
        x = self.conv1(x)
        x = activation_func_map[self.activation_func]()(x)
        x = F.adaptive_avg_pool2d(x, (150, 150))

        x = self.conv2(x)
        x = activation_func_map[self.activation_func]()(x)
        x = F.adaptive_avg_pool2d(x, (75, 75))

        x = x.flatten(start_dim=1)
        x = self.fc1(x)
        x = self.dropout(x)
        x = activation_func_map[self.activation_func]()(x)
        x = self.fc2(x)
        x = torch.sigmoid(x)

        return x

"""---

### 1.4- Model Running Code
"""

def run_experiment(
        model,
        device,
        optimizer,
        criterion,
        augmentations=None,
        transformations=None,
        num_epochs=10,
        batch_size=16
    ):

    train_dataset = MelanomaDataset(DATA_DIR, augmentations=augmentations, transformations=transformations, train=True)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    test_dataset = MelanomaDataset(DATA_DIR, augmentations=None, transformations=transformations, train=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

    # Training loop
    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        correct = 0
        total = 0

        for inputs, labels in tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels.float().view(-1, 1))
            loss.backward()
            optimizer.step()

            train_loss += loss.item() * inputs.size(0)
            predicted = (outputs > 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels.float().view(-1, 1)).sum().item()

        train_loss /= total
        train_accuracy = correct / total
        print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')

    # Testing after training
    model.eval()
    test_loss = 0
    correct = 0
    total = 0

    with torch.no_grad():
        for inputs, labels in tqdm(test_loader, desc='Testing'):
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = model(inputs)
            loss = criterion(outputs, labels.float().view(-1, 1))
            test_loss += loss.item() * inputs.size(0)  # Multiply by batch size
            predicted = (outputs > 0.5).float()
            total += labels.size(0)
            correct += (predicted == labels.float().view(-1, 1)).sum().item()

    test_loss /= total
    test_accuracy = correct / total
    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

"""---

### 1.5- Transformations
"""

mnn_transforms = transforms.Compose([
    transforms.Resize((50, 50)),
    transforms.Grayscale(num_output_channels=1),
    transforms.ToTensor()
])

cnn_transforms = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

"""---

## 2- Experiments

### 2.1 Multi-Layer Neural Network Experiments

- Learning rate: 0.005
- Input size: 50x50
- Activation function: Sigmoid
"""

input_size = 50 * 50
epochs = 10
model = MultiLayerNN(input_size, 'sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=mnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.005
- Input size: 50x50
- Activation function: ReLU
"""

input_size = 50 * 50
epochs = 10
model = MultiLayerNN(input_size, 'relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=mnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.005
- Input size: 300x300
- Activation function: Sigmoid
"""

input_size = 300 * 300
epochs = 10
model = MultiLayerNN(input_size, 'sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
last_two_transforms = transforms.Compose(mnn_transforms.transforms[1:])
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=last_two_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.005
- Input size: 300x300
- Activation function: ReLU
"""

input_size = 300 * 300
epochs = 10
model = MultiLayerNN(input_size, 'relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=last_two_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Input size: 50x50
- Activation function: Sigmoid
"""

input_size = 50 * 50
epochs = 10
model = MultiLayerNN(input_size, 'sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=mnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Input size: 50x50
- Activation function: ReLU
"""

input_size = 50 * 50
epochs = 10
model = MultiLayerNN(input_size, 'relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=mnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Input size: 300x300
- Activation function: Sigmoid
"""

input_size = 300 * 300
epochs = 10
model = MultiLayerNN(input_size, 'sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=last_two_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Input size: 300x300
- Activation function: ReLU
"""

input_size = 300 * 300
epochs = 10
model = MultiLayerNN(input_size, 'relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=last_two_transforms, num_epochs=epochs)

"""---

### 2.2 Convolutional Neural Network Experiments

- Learning rate: 0.005
- Activation function: Sigmoid
- Batch size: 16
"""

epochs = 10
model = ConvNN('sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.005
- Activation function: ReLU
- Batch size: 16
"""

epochs = 10
model = ConvNN('relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.005
- Activation function: Sigmoid
- Batch size: 32
"""

epochs = 10
model = ConvNN('sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs, batch_size=32)

"""---

- Learning rate: 0.005
- Activation function: ReLU
- Batch size: 32
"""

epochs = 10
model = ConvNN('relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.005)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs, batch_size=32)

"""---

- Learning rate: 0.02
- Activation function: Sigmoid
- Batch size: 16
"""

epochs = 10
model = ConvNN('sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Activation function: ReLU
- Batch size: 16
"""

epochs = 10
model = ConvNN('relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs)

"""---

- Learning rate: 0.02
- Activation function: Sigmoid
- Batch size: 32
"""

epochs = 10
model = ConvNN('sigmoid').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs, batch_size=32)

"""---

- Learning rate: 0.02
- Activation function: ReLU
- Batch size: 32
"""

epochs = 10
model = ConvNN('relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.02)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs, batch_size=32)

"""---

### 2.3 My Convolutional Neural Network Experiment

I created a deeper and more complex CNN model based on the observations I made along the way. I actually wanted to try deeper, more complex models than this one but due to lack of resource and time, I am unable to do that.
"""

class MyConvNN(nn.Module):
    def __init__(self, activation_func):
        super(MyConvNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 128, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(128)
        self.conv2 = nn.Conv2d(128, 256, 5, padding=2)
        self.bn2 = nn.BatchNorm2d(256)
        self.conv3 = nn.Conv2d(256, 256, 1)
        self.bn3 = nn.BatchNorm2d(256)
        self.fc1 = nn.Linear(256 * 75 * 75, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 128)
        self.fc3 = nn.Linear(128, 1)
        self.activation_func = activation_func

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = activation_func_map[self.activation_func]()(x)
        x = F.adaptive_avg_pool2d(x, (150, 150))

        x = self.conv2(x)
        x = self.bn2(x)
        x = activation_func_map[self.activation_func]()(x)
        x = F.adaptive_avg_pool2d(x, (75, 75))

        x = self.conv3(x)
        x = self.bn3(x)
        x = activation_func_map[self.activation_func]()(x)

        x = x.flatten(start_dim=1)
        x = self.fc1(x)
        x = self.dropout(x)
        x = activation_func_map[self.activation_func]()(x)

        x = self.fc2(x)
        x = self.dropout(x)
        x = activation_func_map[self.activation_func]()(x)

        x = self.fc3(x)
        x = torch.sigmoid(x)

        return x

epochs = 10
model = MyConvNN('relu').to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.SGD(model.parameters(), lr=0.001)
run_experiment(model, DEVICE, optimizer, criterion, augmentations=None, transformations=cnn_transforms, num_epochs=epochs)

"""---

## 3- Results

| Experiment | Input Size | Model | Activation Function | Hidden Layers | Learning Rate | Batch Size | Accuracy |
| --- | --- | --- | --- | --- | --- | --- | --- |
| 1 | 50*50 | MLP | Sigmoid | 128, 64 | 0.005 | 16 | 0.5659 |
| 2 | 50*50 | MLP | ReLU | 128, 64 | 0.005 | 16 | 0.8329 |
| 3 | 300*300 | MLP | Sigmoid | 128, 64 | 0.005 | 16 | 0.8142 |
| 4 | 300*300 | MLP | ReLU | 128, 64 | 0.005 | 16 | 0.8053 |
| 5 | 50*50 | MLP | Sigmoid | 128, 64 | 0.02 | 16 | 0.7616 |
| 6 | 50*50 | MLP | ReLU | 128, 64 | 0.02 | 16 | 0.5008 |
| 7 | 300*300 | MLP | Sigmoid | 128, 64 | 0.02 | 16 | 0.8048 |
| 8 | 300*300 | MLP | ReLU | 128, 64 | 0.02 | 16 | 0.8220 |
| 9 | 300*300| CNN | Sigmoid | 5x5, 3x3 |0.005 | 16 | 0.4794 |
| 10 | 300*300 | CNN | ReLU | 5x5, 3x3 | 0.005 | 16 | 0.8771 |
| 11 | 300*300 | CNN | Sigmoid | 5x5, 3x3 | 0.005 | 32 | 0.7303 |
| 12 | 300*300 | CNN | ReLU | 5x5, 3x3 | 0.005 | 32 | 0.9073 |
| 13 | 300*300 | CNN | Sigmoid | 5x5, 3x3 | 0.02 | 16 | 0.4794 |
| <span style="color: lightgreen;">14</span> | <span style="color: lightgreen;">300*300</span> | <span style="color: lightgreen;">CNN</span> | <span style="color: lightgreen;">ReLU</span> | <span style="color: lightgreen;">5x5, 3x3</span> | <span style="color: lightgreen;">0.02</span> | <span style="color: lightgreen;">16</span> | <span style="color: lightgreen;">0.9089</span> |
| 15 | 300*300 | CNN | Sigmoid | 5x5, 3x3 | 0.02 | 32 | 0.5206 |
| 16 | 300*300 | CNN | ReLU | 5x5, 3x3 | 0.02 | 32 | 0.9011 |
| <span style="color: lightgreen;">17</span> | <span style="color: lightgreen;">300*300</span> | <span style="color: lightgreen;">CNN</span> | <span style="color: lightgreen;">ReLU</span> | <span style="color: lightgreen;">3x3, 5x5, 1x1</span> | <span style="color: lightgreen;">0.001</span> | <span style="color: lightgreen;">16</span> | <span style="color: lightgreen;">0.9089</span> |

I actually have 2 winners here. By the looks of it, it does not surprise that they both have ReLU as their activation function, since Sigmoid does quite bad job when used in the middle layers. Another common thing is they are both CNN models (which again is not surprising). I actually believe that I can easily get a much better result by using more advanced augmentation and transformation technics, as well as a better model, but due to time and resource limitations I end this assignment here."""